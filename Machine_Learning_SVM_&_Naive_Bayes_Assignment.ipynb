{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5fG69HBKecQ"
      },
      "outputs": [],
      "source": [
        "#Machine Learning-SVM & Naive Bayes Assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is a Support Vector Machine (SVM), and how does it work?\n"
      ],
      "metadata": {
        "id": "bJegQqW_KrFm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression.\n",
        "\n",
        "It works by finding a hyperplane that best separates data points of different classes in a feature space.\n",
        "\n",
        "The best hyperplane is the one that maximizes the margin, i.e., the distance between the hyperplane and the nearest data points from each class (called support vectors).\n",
        "\n",
        "For non-linear problems, SVM uses kernel functions to project data into higher-dimensional space where it becomes linearly separable."
      ],
      "metadata": {
        "id": "9IZKW7X8LCZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: Explain the difference between Hard Margin and Soft Margin SVM."
      ],
      "metadata": {
        "id": "efTi5t1XLHLD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "Hard Margin SVM:\n",
        "\n",
        "Assumes data is perfectly linearly separable.\n",
        "\n",
        "No misclassification is allowed.\n",
        "\n",
        "Sensitive to noise and outliers.\n",
        "\n",
        "Soft Margin SVM:\n",
        "\n",
        "Allows some misclassifications with a penalty (controlled by parameter C).\n",
        "\n",
        "Balances margin maximization with classification error.\n",
        "\n",
        "More robust in real-world noisy datasets."
      ],
      "metadata": {
        "id": "4BzV1QjHLMtk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is the Kernel Trick in SVM? Give one example of a kernel and\n",
        "explain its use case.\n"
      ],
      "metadata": {
        "id": "XnStwgtCLPZF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "The **Kernel Trick** allows SVMs to operate in a high-dimensional feature space without explicitly computing the coordinates of the data in that space. Instead, it computes the inner products between data points in the transformed space using a **kernel function**.\n",
        "\n",
        "**Example — Radial Basis Function (RBF) kernel**:\n",
        "\n",
        "                                    **K(x,x′)=exp(−γ∥x−x′∥2)**\n",
        "Maps data into infinite-dimensional space.\n",
        "\n",
        "Use case: when decision boundaries are non-linear (e.g., classifying spirals or concentric circles)."
      ],
      "metadata": {
        "id": "sIF1p7PpLVpB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What is a Naïve Bayes Classifier, and why is it called “naïve”?\n"
      ],
      "metadata": {
        "id": "gR6Yd7L3MDWj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naïve Bayes is a probabilistic classifier based on **Bayes’ Theorem**:\n",
        "\n",
        "P(y∣X)=P(X∣y)P(y)/P(X)\n",
        "\n",
        "It assumes conditional independence between features given the class label.\n",
        "\n",
        "Called “naïve” because in real-world data features are often correlated, but the model still performs surprisingly well in many tasks, especially text classification.\n",
        "\t​\n"
      ],
      "metadata": {
        "id": "5RHluiYaMGXJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants.\n",
        "When would you use each one?\n"
      ],
      "metadata": {
        "id": "SPWME09_M2By"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "**Gaussian Naïve Bayes:**\n",
        "\n",
        "Assumes features follow a normal (Gaussian) distribution.\n",
        "\n",
        "Use case: continuous data like Iris flower measurements.\n",
        "\n",
        "**Multinomial Naïve Bayes:**\n",
        "\n",
        "Assumes features are counts/frequencies.\n",
        "\n",
        "Use case: text classification, word counts in documents.\n",
        "\n",
        "**Bernoulli Naïve Bayes:**\n",
        "\n",
        "Assumes binary features (0/1).\n",
        "\n",
        "Use case: text classification with presence/absence of words (spam detection)."
      ],
      "metadata": {
        "id": "zp-ZR-rBM5u7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Write a Python program to:\n",
        "\n",
        "● Load the Iris dataset\n",
        "\n",
        "● Train an SVM Classifier with a linear kernel\n",
        "\n",
        "● Print the model's accuracy and support vectors.\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "sW7jz3dDNWzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Train SVM with linear kernel\n",
        "clf = SVC(kernel=\"linear\", random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Accuracy\n",
        "y_pred = clf.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {acc:.4f}\")\n",
        "print(\"Support Vectors:\")\n",
        "print(clf.support_vectors_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GeOjU-OINf4g",
        "outputId": "d38b7fde-a512-4b2c-96bb-e7cddef025b4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0000\n",
            "Support Vectors:\n",
            "[[4.8 3.4 1.9 0.2]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [5.1 3.8 1.9 0.4]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [6.2 2.2 4.5 1.5]\n",
            " [6.  2.9 4.5 1.5]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.7 3.1 4.7 1.5]\n",
            " [6.8 2.8 4.8 1.4]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.5 3.2 5.1 2. ]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.3 2.5 5.  1.9]\n",
            " [6.  2.2 5.  1.5]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [7.2 3.  5.8 1.6]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "\n",
        "● Load the Breast Cancer dataset\n",
        "\n",
        "● Train a Gaussian Naïve Bayes model\n",
        "\n",
        "● Print its classification report including precision, recall, and F1-score.\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "8WJjS48zNiui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load data\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Train Gaussian NB\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Predict & report\n",
        "y_pred = gnb.predict(X_test)\n",
        "print(classification_report(y_test, y_pred, target_names=cancer.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxgQ7U4FNsvh",
        "outputId": "07cb9bc2-ffd1-4adf-b754-46d3a249c288"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       0.97      0.89      0.93        64\n",
            "      benign       0.94      0.98      0.96       107\n",
            "\n",
            "    accuracy                           0.95       171\n",
            "   macro avg       0.95      0.94      0.94       171\n",
            "weighted avg       0.95      0.95      0.95       171\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "\n",
        "● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best C and gamma.\n",
        "\n",
        "● Print the best hyperparameters and accuracy.\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "_hBvJKBANumx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Grid Search\n",
        "param_grid = {\"C\": [0.1, 1, 10], \"gamma\": [0.001, 0.01, 0.1, 1]}\n",
        "grid = GridSearchCV(SVC(kernel=\"rbf\"), param_grid, cv=5, scoring=\"accuracy\")\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "print(\"Best parameters:\", grid.best_params_)\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFawlQ3AN-1b",
        "outputId": "65a04a01-6373-40a0-fdcf-1073fc0ca229"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'C': 10, 'gamma': 0.001}\n",
            "Test Accuracy: 0.7777777777777778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "\n",
        "● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "sklearn.datasets.fetch_20newsgroups).\n",
        "\n",
        "● Print the model's ROC-AUC score for its predictions.\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "lSmbuD51OA1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Load dataset (subset for speed)\n",
        "data = fetch_20newsgroups(subset=\"train\", categories=['sci.space', 'rec.sport.baseball'], remove=('headers','footers','quotes'))\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Vectorize text\n",
        "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=5000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Train Naive Bayes\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_prob = nb.predict_proba(X_test_tfidf)\n",
        "roc_auc = roc_auc_score(y_test, y_prob[:,1])\n",
        "\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnLH4KHLOKn5",
        "outputId": "0e49171d-192b-4c26-bce6-c50c0bebcfb7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9909\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you’re working as a data scientist for a company that handles email communications.\n",
        "\n",
        "Your task is to automatically classify emails as Spam or Not Spam. The emails may contain:\n",
        "\n",
        "● Text with diverse vocabulary\n",
        "\n",
        "● Potential class imbalance (far more legitimate emails than spam)\n",
        "\n",
        "● Some incomplete or missing data\n",
        "\n",
        "Explain the approach you would take to:\n",
        "\n",
        "● Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "\n",
        "● Choose and justify an appropriate model (SVM vs. Naïve Bayes)\n",
        "\n",
        "● Address class imbalance\n",
        "\n",
        "● Evaluate the performance of your solution with suitable metrics\n",
        "\n",
        "And explain the business impact of your solution.\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "5HlbWsWWOMv9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "Approach:\n",
        "\n",
        "Preprocessing:\n",
        "\n",
        "Handle missing emails (drop or impute missing body).\n",
        "\n",
        "Convert text to numerical features using TfidfVectorizer or CountVectorizer.\n",
        "\n",
        "Optionally use n-grams (bi-grams) for more context.\n",
        "\n",
        "Model Choice:\n",
        "\n",
        "Naïve Bayes is usually preferred for spam detection (fast, works well with text).\n",
        "\n",
        "SVM can be used for higher accuracy but is slower on large datasets.\n",
        "\n",
        "Class Imbalance:\n",
        "\n",
        "Use class_weight='balanced' in SVM.\n",
        "\n",
        "Or use oversampling (SMOTE) / undersampling techniques.\n",
        "\n",
        "Use stratified train-test splits.\n",
        "\n",
        "Evaluation Metrics:\n",
        "\n",
        "Accuracy alone is misleading (due to imbalance).\n",
        "\n",
        "Use Precision, Recall, F1-score, ROC-AUC.\n",
        "\n",
        "Recall is critical (don’t miss spam).\n",
        "\n",
        "Business Impact:\n",
        "\n",
        "Reduces manual filtering effort.\n",
        "\n",
        "Protects employees/customers from phishing and malware.\n",
        "\n",
        "Improves productivity and trust in company email system."
      ],
      "metadata": {
        "id": "-AelRTfBOiny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "# Load subset as spam vs not spam (simulate with 2 categories)\n",
        "data = fetch_20newsgroups(subset=\"train\", categories=[\"rec.sport.hockey\", \"sci.med\"], remove=(\"headers\",\"footers\",\"quotes\"))\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Vectorize\n",
        "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=5000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Train Naive Bayes\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = nb.predict(X_test_tfidf)\n",
        "y_prob = nb.predict_proba(X_test_tfidf)[:,1]\n",
        "\n",
        "# Evaluation\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test, y_prob))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fbkb8YbEOqX0",
        "outputId": "f7755815-697e-424c-8cbc-bea73de91ed8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.96      0.96       180\n",
            "           1       0.96      0.97      0.96       179\n",
            "\n",
            "    accuracy                           0.96       359\n",
            "   macro avg       0.96      0.96      0.96       359\n",
            "weighted avg       0.96      0.96      0.96       359\n",
            "\n",
            "ROC-AUC: 0.9964928615766604\n"
          ]
        }
      ]
    }
  ]
}